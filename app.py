#!/usr/bin/env python3
"""
Betfa.com Opener - GitHub Codespace Version
Using requests + BeautifulSoup (No Browser Needed)
"""

import requests
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime

def get_website_content(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ Ø³Ø§ÛŒØª Ø¨Ø§ requests"""
    print(f"ğŸŒ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ: {url}")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        
        print(f"âœ… ÙˆØ¶Ø¹ÛŒØª: {response.status_code}")
        return response.text
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø³Ø§ÛŒØª: {e}")
        return None

def extract_content(html_content):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ Ø§Ø² HTML"""
    print("ğŸ” Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§...")
    
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # Ø­Ø°Ù Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ Ùˆ Ø§Ø³ØªØ§ÛŒÙ„â€ŒÙ‡Ø§
    for script in soup(["script", "style", "meta", "link"]):
        script.decompose()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù…
    page_data = {
        'title': soup.title.string if soup.title else 'No Title',
        'text_content': soup.get_text(separator='\n', strip=True),
        'links': [a.get('href') for a in soup.find_all('a', href=True)],
        'images': [img.get('src') for img in soup.find_all('img', src=True)]
    }
    
    return page_data

def save_content(page_data, url):
    """Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§ Ø¯Ø± ÙØ§ÛŒÙ„"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† Ø®Ø§Ù„Øµ
    with open(f"betfa_content_{timestamp}.txt", "w", encoding="utf-8") as f:
        f.write(f"URL: {url}\n")
        f.write(f"Title: {page_data['title']}\n")
        f.write(f"Time: {datetime.now()}\n")
        f.write("=" * 50 + "\n")
        f.write(page_data['text_content'])
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    with open(f"betfa_links_{timestamp}.txt", "w", encoding="utf-8") as f:
        for link in page_data['links']:
            f.write(link + "\n")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª JSON
    with open(f"betfa_data_{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump(page_data, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ Ù…Ø­ØªÙˆØ§ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯:")
    print(f"   ğŸ“„ betfa_content_{timestamp}.txt")
    print(f"   ğŸ”— betfa_links_{timestamp}.txt")
    print(f"   ğŸ“Š betfa_data_{timestamp}.json")

def display_preview(page_data):
    """Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ù…Ø­ØªÙˆØ§"""
    print("\n" + "=" * 60)
    print("ğŸ“Š Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ù…Ø­ØªÙˆØ§ÛŒ Ø³Ø§ÛŒØª")
    print("=" * 60)
    
    print(f"ğŸ“„ Ø¹Ù†ÙˆØ§Ù†: {page_data['title']}")
    print(f"ğŸ“ Ø·ÙˆÙ„ Ù…ØªÙ†: {len(page_data['text_content'])} Ú©Ø§Ø±Ø§Ú©ØªØ±")
    print(f"ğŸ”— ØªØ¹Ø¯Ø§Ø¯ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {len(page_data['links'])}")
    print(f"ğŸ–¼ï¸ ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ±: {len(page_data['images'])}")
    
    print("\nğŸ“ Ø¨Ø®Ø´ÛŒ Ø§Ø² Ù…Ø­ØªÙˆØ§:")
    lines = page_data['text_content'].split('\n')
    for line in lines[:20]:  # 20 Ø®Ø· Ø§ÙˆÙ„
        if line.strip() and len(line.strip()) > 10:
            print(f"   {line[:100]}..." if len(line) > 100 else f"   {line}")
    
    print("\nğŸ”— Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…:")
    for link in page_data['links'][:10]:  # 10 Ù„ÛŒÙ†Ú© Ø§ÙˆÙ„
        if link and not link.startswith(('javascript:', '#')):
            print(f"   {link}")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    print("ğŸš€ Ø³ÛŒØ³ØªÙ… Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ Betfa.com")
    print("ğŸ”¥ Ù†Ø³Ø®Ù‡ Ù…Ø®ØµÙˆØµ GitHub Codespace")
    print("=" * 50)
    
    # Ø¢Ø¯Ø±Ø³ Ø³Ø§ÛŒØª
    target_url = "https://betfa.com/home/index"
    
    # Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§
    html_content = get_website_content(target_url)
    
    if html_content:
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§
        page_data = extract_content(html_content)
        
        # Ù†Ù…Ø§ÛŒØ´ Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´
        display_preview(page_data)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø­ØªÙˆØ§
        save_content(page_data, target_url)
        
        print("\nğŸ‰ Ø¹Ù…Ù„ÛŒØ§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print("ğŸ“‹ Ø§Ú©Ù†ÙˆÙ† Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯:")
        print("   â€¢ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ txt Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯")
        print("   â€¢ ØªÙ…Ø§Ù… Ù…Ø­ØªÙˆØ§ Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒØ¯")
        print("   â€¢ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯")
        print("   â€¢ Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ú©Ø§Ø± Ú©Ù†ÛŒØ¯")
        
    else:
        print("âŒ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ Ø³Ø§ÛŒØª Ø¯Ø³ØªØ±Ø³ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯")
        print("ğŸ’¡ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ: Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø³Ø§ÛŒØª Ù…Ø³Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù‡ Ø¨Ø§Ø´Ø¯")

if __name__ == "__main__":
    main()
